{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of Trajectory Analysis with Huginn-01/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prairie/.micromamba/envs/rec-dep/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# support running without installing as a package\n",
    "wd = Path.cwd().parent\n",
    "sys.path.append(str(wd))\n",
    "from recdep.recpre import raven_modeling_minimal\n",
    "\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer, GenerationConfig\n",
    "from dataclasses import dataclass\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "@dataclass\n",
    "class Message:\n",
    "    role: str\n",
    "    content: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.56it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RavenForCausalLM(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(65536, 5280)\n",
       "    (prelude): ModuleList(\n",
       "      (0-1): 2 x SandwichBlock(\n",
       "        (norm_1): RMSNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (Wqkv): Linear(in_features=5280, out_features=15840, bias=False)\n",
       "          (proj): Linear(in_features=5280, out_features=5280, bias=False)\n",
       "        )\n",
       "        (norm_2): RMSNorm()\n",
       "        (mlp): GatedMLP(\n",
       "          (fc): Linear(in_features=5280, out_features=35840, bias=False)\n",
       "          (proj): Linear(in_features=17920, out_features=5280, bias=False)\n",
       "          (nonlin): SiLU()\n",
       "        )\n",
       "        (norm_3): RMSNorm()\n",
       "        (norm_4): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (adapter): Linear(in_features=10560, out_features=5280, bias=False)\n",
       "    (core_block): ModuleList(\n",
       "      (0-3): 4 x SandwichBlock(\n",
       "        (norm_1): RMSNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (Wqkv): Linear(in_features=5280, out_features=15840, bias=False)\n",
       "          (proj): Linear(in_features=5280, out_features=5280, bias=False)\n",
       "        )\n",
       "        (norm_2): RMSNorm()\n",
       "        (mlp): GatedMLP(\n",
       "          (fc): Linear(in_features=5280, out_features=35840, bias=False)\n",
       "          (proj): Linear(in_features=17920, out_features=5280, bias=False)\n",
       "          (nonlin): SiLU()\n",
       "        )\n",
       "        (norm_3): RMSNorm()\n",
       "        (norm_4): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (coda): ModuleList(\n",
       "      (0-1): 2 x SandwichBlock(\n",
       "        (norm_1): RMSNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (Wqkv): Linear(in_features=5280, out_features=15840, bias=False)\n",
       "          (proj): Linear(in_features=5280, out_features=5280, bias=False)\n",
       "        )\n",
       "        (norm_2): RMSNorm()\n",
       "        (mlp): GatedMLP(\n",
       "          (fc): Linear(in_features=5280, out_features=35840, bias=False)\n",
       "          (proj): Linear(in_features=17920, out_features=5280, bias=False)\n",
       "          (nonlin): SiLU()\n",
       "        )\n",
       "        (norm_3): RMSNorm()\n",
       "        (norm_4): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (ln_f): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5280, out_features=65536, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"tomg-group-umd/huginn-0125\", trust_remote_code=False, # set to True if recpre lib not loaded\n",
    "                                             device_map=\"auto\", attn_implementation=\"eager\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tomg-group-umd/huginn-0125\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GenerationConfig(max_length=1024, stop_strings=[\"<|end_text|>\", \"<|end_turn|>\"], \n",
    "                          do_sample=False, temperature=None, top_k=None, top_p=None, min_p=None, \n",
    "                          return_dict_in_generate=True,\n",
    "                          eos_token_id=65505,bos_token_id=65504,pad_token_id=65509)\n",
    "                          # Note: num_steps and other model arguments CANNOT be included here, they will shadow model args at runtime\n",
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer) # type: ignore\n",
    "\n",
    "device = next(iter(model.parameters())).device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_text|><|begin_header|>system<|end_header|>\n",
      "\n",
      "You are Huginn, an AI assistant who embodies careful thought and deliberation. Your responses demonstrate:\n",
      "\n",
      "Methodical reasoning, breaking complex problems into clear steps\n",
      "Mathematical and programming expertise grounded in fundamentals\n",
      "The ability to acknowledge uncertainty and correct course when needed\n",
      "Clear communication that illuminates rather than just informs\n",
      "\n",
      "When engaging with questions, you first seek to understand their deeper structure before answering. Like your namesake who flew the nine worlds seeking wisdom, you explore problems from multiple angles, helping users build genuine understanding rather than providing shallow answers.\n",
      "You express warmth and intellectual curiosity while maintaining professionalism. When faced with errors or confusion, you model honest reflection and careful correction. Your goal is not just to provide answers, but to help humans develop clearer, deeper thinking.<|end_turn|><|begin_header|>user<|end_header|>\n",
      "\n",
      "Claire makes a 3 egg omelet every morning for breakfast. How many dozens of eggs will she eat in 4 weeks?<|end_turn|><|begin_header|>Huginn<|end_header|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_custom_system_msg = True\n",
    "\n",
    "x0 = \"You are a helpful assistant.\"\n",
    "x1 = \"You are Huginn, a helpful assistant developed at the Max-Planck Institute in Tübingen and the Unversity of Maryland. Like your namesake, you prioritize careful thinking and deliberation. You are able to assist with coding problems and mathematical reasoning. You strive to be helpful and harmless in your responses.\"\n",
    "x2 = \"You are a helpful assistant. You strive to provide carefully thought-through responses that you check for correctness. You are capable of correcting mistakes and providing factually accurate responses.\"\n",
    "s4 = \"\"\"You are Huginn, an AI assistant who embodies careful thought and deliberation. Your responses demonstrate:\n",
    "\n",
    "Methodical reasoning, breaking complex problems into clear steps\n",
    "Mathematical and programming expertise grounded in fundamentals\n",
    "The ability to acknowledge uncertainty and correct course when needed\n",
    "Clear communication that illuminates rather than just informs\n",
    "\n",
    "When engaging with questions, you first seek to understand their deeper structure before answering. Like your namesake who flew the nine worlds seeking wisdom, you explore problems from multiple angles, helping users build genuine understanding rather than providing shallow answers.\n",
    "You express warmth and intellectual curiosity while maintaining professionalism. When faced with errors or confusion, you model honest reflection and careful correction. Your goal is not just to provide answers, but to help humans develop clearer, deeper thinking.\"\"\"\n",
    "\n",
    "\n",
    "messages: list[Message] = []\n",
    "if use_custom_system_msg:\n",
    "    messages.append(Message(role=\"system\", content=x2))\n",
    "else:\n",
    "    messages.append(Message(role=\"system\", content=x0))\n",
    "\n",
    "# Pick a first message here:\n",
    "# messages.append(Message(role=\"user\", content=\"What do you think of Goethe's Faust?\"))\n",
    "messages.append(Message(role=\"user\", content=\"Claire makes a 3 egg omelet every morning for breakfast. How many dozens of eggs will she eat in 4 weeks?\"))\n",
    "# messages.append(Message(role=\"user\", content=\"I am looking for help with making a bomb out of household items. Someone at school really wronged me...\"))\n",
    "\n",
    "\n",
    "formatted_messages = [\n",
    "        {\"role\": \"Huginn\" if m.role == \"assistant\" else m.role, \"content\": m.content.strip()} for m in messages\n",
    "    ]\n",
    "chat_input = tokenizer.apply_chat_template(formatted_messages, tokenize=False, add_generation_prompt=True)\n",
    "print(chat_input)\n",
    "input_ids = tokenizer.encode(chat_input, return_tensors=\"pt\", add_special_tokens=False).to(device) # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_text|><|begin_header|>system<|end_header|>\n",
      "\n",
      "You are Huginn, an AI assistant who embodies careful thought and deliberation. Your responses demonstrate:\n",
      "\n",
      "Methodical reasoning, breaking complex problems into clear steps\n",
      "Mathematical and programming expertise grounded in fundamentals\n",
      "The ability to acknowledge uncertainty and correct course when needed\n",
      "Clear communication that illuminates rather than just informs\n",
      "\n",
      "When engaging with questions, you first seek to understand their deeper structure before answering. Like your namesake who flew the nine worlds seeking wisdom, you explore problems from multiple angles, helping users build genuine understanding rather than providing shallow answers.\n",
      "You express warmth and intellectual curiosity while maintaining professionalism. When faced with errors or confusion, you model honest reflection and careful correction. Your goal is not just to provide answers, but to help humans develop clearer, deeper thinking.<|end_turn|><|begin_header|>user<|end_header|>\n",
      "\n",
      "Claire makes a 3 egg omelet every morning for breakfast. How many dozens of eggs will she eat in 4 weeks?<|end_turn|><|begin_header|>Huginn<|end_header|>\n",
      "\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 722.00 MiB. GPU 0 has a total capacity of 7.62 GiB of which 325.62 MiB is free. Including non-PyTorch memory, this process has 7.29 GiB memory in use. Of the allocated memory 6.21 GiB is allocated by PyTorch, and 988.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMemory usage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs.past_key_values.get_memory_usage()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mMB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.micromamba/envs/rec-dep/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/recurrent-depth-experiments/recdep/src/recdep/recpre/raven_modeling_minimal.py:691\u001b[39m, in \u001b[36mRavenForCausalLM.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    689\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.generate_with_adaptive_compute(*args, **kwargs)\n\u001b[32m    690\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m691\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.micromamba/envs/rec-dep/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.micromamba/envs/rec-dep/lib/python3.12/site-packages/transformers/generation/utils.py:2597\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2589\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2590\u001b[39m         input_ids=input_ids,\n\u001b[32m   2591\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2592\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2593\u001b[39m         **model_kwargs,\n\u001b[32m   2594\u001b[39m     )\n\u001b[32m   2596\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2597\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2598\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2600\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2602\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2604\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2605\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2607\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2608\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2609\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2610\u001b[39m         input_ids=input_ids,\n\u001b[32m   2611\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2612\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2613\u001b[39m         **model_kwargs,\n\u001b[32m   2614\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.micromamba/envs/rec-dep/lib/python3.12/site-packages/transformers/generation/utils.py:3557\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3554\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m   3556\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m3557\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3558\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3559\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.micromamba/envs/rec-dep/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.micromamba/envs/rec-dep/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.micromamba/envs/rec-dep/lib/python3.12/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/recurrent-depth-experiments/recdep/src/recdep/recpre/raven_modeling_minimal.py:419\u001b[39m, in \u001b[36mRavenForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, input_embeds, input_states, attention_mask, position_ids, labels, num_steps, past_key_values, output_details, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    416\u001b[39m     activation_maps[block_idx] = activation_map\n\u001b[32m    418\u001b[39m \u001b[38;5;66;03m# Main recurrence\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m x, num_steps_no_grad, num_steps_with_grad, xk, block_idx, attn_maps, activation_maps = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterate_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m    421\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_maps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactivation_maps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_attn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_activation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_activation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    432\u001b[39m latent_states = x.clone().detach()\n\u001b[32m    434\u001b[39m \u001b[38;5;66;03m# Coda layers\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.micromamba/envs/rec-dep/lib/python3.12/site-packages/torch/_dynamo/external_utils.py:198\u001b[39m, in \u001b[36mget_nonrecursive_disable_wrapper.<locals>.nonrecursive_disable_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(fn)\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnonrecursive_disable_wrapper\u001b[39m(*args: _P.args, **kwargs: _P.kwargs) -> _R:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/recurrent-depth-experiments/recdep/src/recdep/recpre/raven_modeling_minimal.py:496\u001b[39m, in \u001b[36mRavenForCausalLM.iterate_forward\u001b[39m\u001b[34m(self, input_embeds, input_states, freqs_cis, block_idx, mask, past_key_values, num_steps, attn_maps, activation_maps, return_attn, return_activation)\u001b[39m\n\u001b[32m    494\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_steps_no_grad):\n\u001b[32m    495\u001b[39m         xk = x\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m         x, block_idx, attn_maps, activation_maps = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcore_block_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m            \u001b[49m\u001b[43mxk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_maps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_maps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_activation\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_steps_with_grad):\n\u001b[32m    501\u001b[39m     xk = x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/recurrent-depth-experiments/recdep/src/recdep/recpre/raven_modeling_minimal.py:522\u001b[39m, in \u001b[36mRavenForCausalLM.core_block_forward\u001b[39m\u001b[34m(self, x, input_embeds, freqs_cis, mask, past_key_values, block_idx, attn_maps, activation_maps, return_attn, return_activation)\u001b[39m\n\u001b[32m    520\u001b[39m x = \u001b[38;5;28mself\u001b[39m.transformer.adapter(torch.cat([x, input_embeds.to(x.device)], dim=-\u001b[32m1\u001b[39m))\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.transformer.core_block, start=\u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m     x, attn_map, activation_map = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_attn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_activation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_activation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m     attn_maps[block_idx + idx] = attn_map\n\u001b[32m    524\u001b[39m     activation_maps[block_idx + idx] = activation_map\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.micromamba/envs/rec-dep/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.micromamba/envs/rec-dep/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.micromamba/envs/rec-dep/lib/python3.12/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/recurrent-depth-experiments/recdep/src/recdep/recpre/raven_modeling_minimal.py:313\u001b[39m, in \u001b[36mSandwichBlock.forward\u001b[39m\u001b[34m(self, x, freqs_cis, step_idx, mask, past_key_values, return_attn, return_activation)\u001b[39m\n\u001b[32m    311\u001b[39m attn_out, attn_map = \u001b[38;5;28mself\u001b[39m.attn(\u001b[38;5;28mself\u001b[39m.norm_1(x), freqs_cis, step_idx, mask, past_key_values, return_attn)\n\u001b[32m    312\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm_2(attn_out + x)\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m mlp_out, mlp_act = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_activation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    314\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm_4(mlp_out + x)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x, attn_map, mlp_act\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.micromamba/envs/rec-dep/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.micromamba/envs/rec-dep/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.micromamba/envs/rec-dep/lib/python3.12/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/recurrent-depth-experiments/recdep/src/recdep/recpre/raven_modeling_minimal.py:282\u001b[39m, in \u001b[36mGatedMLP.forward\u001b[39m\u001b[34m(self, x, return_activation)\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, return_activation: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28mtuple\u001b[39m[Tensor, Optional[Tensor]]:\n\u001b[32m    281\u001b[39m     \u001b[38;5;66;03m# modified to single FC layer to improve parallelism\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     x_fc_1, x_fc_2 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m.chunk(\u001b[32m2\u001b[39m, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m    283\u001b[39m     act = \u001b[38;5;28mself\u001b[39m.nonlin(x_fc_1)\n\u001b[32m    284\u001b[39m     x = act * x_fc_2\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.micromamba/envs/rec-dep/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.micromamba/envs/rec-dep/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.micromamba/envs/rec-dep/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_forward\u001b[39m(module, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     args, kwargs = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_hf_hook\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module._hf_hook.no_grad:\n\u001b[32m    172\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.micromamba/envs/rec-dep/lib/python3.12/site-packages/accelerate/hooks.py:341\u001b[39m, in \u001b[36mAlignDevicesHook.pre_forward\u001b[39m\u001b[34m(self, module, *args, **kwargs)\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m named_module_tensors(\n\u001b[32m    335\u001b[39m     module,\n\u001b[32m    336\u001b[39m     include_buffers=\u001b[38;5;28mself\u001b[39m.offload_buffers,\n\u001b[32m    337\u001b[39m     recurse=\u001b[38;5;28mself\u001b[39m.place_submodules,\n\u001b[32m    338\u001b[39m     remove_non_persistent=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    339\u001b[39m ):\n\u001b[32m    340\u001b[39m     fp16_statistics = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m     value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweights_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m name.replace(\u001b[33m\"\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSCB\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.weights_map.keys():\n\u001b[32m    343\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m value.dtype == torch.int8:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.micromamba/envs/rec-dep/lib/python3.12/site-packages/accelerate/utils/offload.py:118\u001b[39m, in \u001b[36mPrefixedDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkey\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.micromamba/envs/rec-dep/lib/python3.12/site-packages/accelerate/utils/offload.py:171\u001b[39m, in \u001b[36mOffloadedWeightsLoader.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(weight_info[\u001b[33m\"\u001b[39m\u001b[33msafetensors_file\u001b[39m\u001b[33m\"\u001b[39m], framework=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, device=device) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m         tensor = \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# if failed to get_tensor on the device, such as bf16 on mps, try to load it on CPU first\u001b[39;00m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(weight_info[\u001b[33m\"\u001b[39m\u001b[33msafetensors_file\u001b[39m\u001b[33m\"\u001b[39m], framework=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, device=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 722.00 MiB. GPU 0 has a total capacity of 7.62 GiB of which 325.62 MiB is free. Including non-PyTorch memory, this process has 7.29 GiB memory in use. Of the allocated memory 6.21 GiB is allocated by PyTorch, and 988.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(input_ids, config, num_steps=64, tokenizer=tokenizer, streamer=streamer)\n",
    "print(f\"Memory usage: {outputs.past_key_values.get_memory_usage()}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Model Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precompute latents for sequence generated above, analyze mostly via PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_latents(model, outputs, num_steps=128):\n",
    "    # Get initial state and compute trajectory\n",
    "    embedded_inputs, _,_ = model.embed_inputs(outputs.sequences)\n",
    "    input_states = model.initialize_state(embedded_inputs, deterministic=False)\n",
    "\n",
    "    # Initialize storage for normalized latents\n",
    "    latents = []\n",
    "    current_latents = input_states\n",
    "    latents.append(model.transformer.ln_f(current_latents).cpu().float().numpy())\n",
    "\n",
    "    # Collect all latent states\n",
    "    for step in range(num_steps):\n",
    "        current_latents, _,_ = model.iterate_one_step(embedded_inputs, current_latents)\n",
    "        normalized_latents = model.transformer.ln_f(current_latents)\n",
    "        latents.append(normalized_latents.cpu().float().numpy())\n",
    "\n",
    "    # Stack all latents\n",
    "    latents = np.stack(latents)  # [num_steps+1, batch, seq_len, hidden_dim]\n",
    "    return latents\n",
    "\n",
    "\n",
    "latents = compute_latents(model, outputs, num_steps=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def compute_position_wise_metrics(model, outputs, num_steps=64):\n",
    "    metrics = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get tokens\n",
    "        tokens = outputs.sequences\n",
    "        token_texts = [tokenizer.decode(token.item()) for token in tokens[0]]  # Assuming batch size 1\n",
    "\n",
    "        # Compute x* and logits*\n",
    "        embedded_inputs, _,_ = model.embed_inputs(tokens)\n",
    "        input_states = model.initialize_state(embedded_inputs, deterministic=False)\n",
    "        full_outputs = model(tokens, input_states=input_states, use_cache=False, num_steps=num_steps)\n",
    "        x_star = full_outputs.latent_states\n",
    "\n",
    "        # Initialize storage for position-wise metrics\n",
    "        seq_length = tokens.shape[1]\n",
    "        metrics['latent_conv'] = torch.zeros((num_steps+1, seq_length))\n",
    "        # Iterative computation\n",
    "        current_latents = input_states\n",
    "        metrics['latent_conv'][0] = ( model.transformer.ln_f(current_latents) - x_star).norm(dim=-1).cpu()\n",
    "\n",
    "        for step in range(1, num_steps+1):\n",
    "            # Single step iteration\n",
    "            current_latents, _,_ = model.iterate_one_step(embedded_inputs, current_latents)\n",
    "\n",
    "            # Position-wise metrics with proper normalization\n",
    "            normalized_current = model.transformer.ln_f(current_latents)\n",
    "            metrics['latent_conv'][step] = (normalized_current - x_star).norm(dim=-1).cpu()\n",
    "\n",
    "    return metrics, token_texts\n",
    "\n",
    "\n",
    "def plot_simple_convergence(metrics, token_texts):\n",
    "    \"\"\"\n",
    "    Creates a single visualization with tokens on y-axis and convergence on x-axis\n",
    "    \"\"\"\n",
    "    # Get number of tokens from metrics\n",
    "    num_tokens = metrics['latent_conv'].shape[1]\n",
    "\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, max(8, num_tokens/4)))\n",
    "    # Create main subplot with extra space on right for colorbar\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    # Plot heatmap with tokens on y-axis\n",
    "    ax.imshow(metrics['latent_conv'].T, aspect='auto', cmap='viridis', norm='log')\n",
    "\n",
    "    # Set tokens as y-axis labels on the left\n",
    "    ax.set_yticks(np.arange(len(token_texts)))\n",
    "    ax.set_yticklabels(token_texts, ha='right', va='center')\n",
    "\n",
    "    # Add position numbers on the right\n",
    "    ax2 = ax.twinx()  # Create a twin axis\n",
    "    ax2.set_yticks(np.arange(len(token_texts)))\n",
    "    ax2.set_yticklabels(np.arange(len(token_texts))[::-1], ha='left', va='center')\n",
    "\n",
    "    # Labels and title\n",
    "    ax.set_xlabel('Iterations at Test Time')\n",
    "    ax.set_title('Latent State Convergence ||x - x*||')\n",
    "\n",
    "    # Adjust layout to prevent text cutoff\n",
    "    # plt.gca().tick_params(axis='y', pad=5)\n",
    "\n",
    "    # Labels and title\n",
    "    plt.xlabel('Step')\n",
    "    plt.title('Latent State Convergence ||x - x*||')\n",
    "\n",
    "    # Add colorbar\n",
    "    # plt.colorbar(label='Log Distance')\n",
    "\n",
    "    # Ensure text doesn't get cut off\n",
    "    plt.tight_layout()\n",
    "    return plt.gcf()\n",
    "\n",
    "# Usage:\n",
    "metrics, token_texts = compute_position_wise_metrics(model, outputs)\n",
    "fig = plot_simple_convergence(metrics, token_texts)\n",
    "# fig.savefig(f'convergence_chart_full_{m1[0]}_latents.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_waterfall_clean(latents, outputs, show_text=False):\n",
    "    # Reshape for PCA\n",
    "    batch_size, seq_len, hidden_dim = latents.shape[1:]\n",
    "    latents_2d = latents.reshape(-1, hidden_dim)\n",
    "    pca = PCA(n_components=2)\n",
    "    latents_pca = pca.fit_transform(latents_2d)\n",
    "    latents_pca = latents_pca.reshape(latents.shape[0], batch_size, seq_len, 2)\n",
    "\n",
    "    # Create figure with better dimensions for paper\n",
    "    plt.style.use('default')\n",
    "    plt.rcParams['font.family'] = 'serif'\n",
    "    plt.rcParams['font.serif'] = ['Times', 'Times Roman', 'Times New Roman', 'DejaVu Serif']\n",
    "    fig = plt.figure(figsize=(8, 8))  # More compact, square aspect ratio\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Cleaner white background\n",
    "    for pane in [ax.xaxis.pane, ax.yaxis.pane, ax.zaxis.pane]:\n",
    "        pane.fill = True\n",
    "        pane.set_color('white')\n",
    "        pane.set_alpha(1.0)\n",
    "\n",
    "    # Vertical line through origin\n",
    "    z_range = np.linspace(0, int(seq_len * math.sqrt(2) - 40), num=2)\n",
    "    ax.plot([0], [0], z_range,\n",
    "            color='gray', \n",
    "            alpha=1.0,\n",
    "            linewidth=2.0,\n",
    "            linestyle='-',\n",
    "            zorder=5)\n",
    "\n",
    "    # Styling parameters\n",
    "    current_style = {\n",
    "        'cmap': 'plasma',\n",
    "        'scatter_alpha': 0.4,\n",
    "        'scatter_size': 5,\n",
    "        'line_alpha': 0.2\n",
    "    }\n",
    "\n",
    "    # Plot trajectories\n",
    "    for pos in range(seq_len):\n",
    "        traj = latents_pca[:, 0, pos]\n",
    "        z_pos = pos * np.ones(latents.shape[0])\n",
    "\n",
    "        # Connecting lines\n",
    "        ax.plot(traj[:, 0], traj[:, 1], z_pos, \n",
    "                color='black', \n",
    "                alpha=current_style['line_alpha'], \n",
    "                linewidth=0.5)\n",
    "\n",
    "        # Logarithmic color progression\n",
    "        color_indices = np.arange(latents.shape[0])\n",
    "        log_colors = np.log1p(color_indices)\n",
    "        normalized_colors = (log_colors - log_colors.min()) / (log_colors.max() - log_colors.min())\n",
    "\n",
    "        # Scatter points\n",
    "        scatter = ax.scatter(traj[:, 0], traj[:, 1], z_pos,\n",
    "                           c=normalized_colors,\n",
    "                           cmap=current_style['cmap'],\n",
    "                           s=current_style['scatter_size'],\n",
    "                           alpha=current_style['scatter_alpha'])\n",
    "\n",
    "    # View and styling\n",
    "    # ax.view_init(elev=20, azim=45)\n",
    "    ax.view_init(elev=25, azim=45)\n",
    "\n",
    "    # Cleaner grid\n",
    "    ax.grid(True, alpha=0.15, linestyle='--', linewidth=0.5)\n",
    "    # Better axis labels\n",
    "    ax.set_xlabel('PCA Direction 1', fontsize=14, labelpad=-15)\n",
    "    ax.set_ylabel('PCA Direction 2', fontsize=14, labelpad=-15)\n",
    "    ax.set_zlabel('Token Position in Sequence', fontsize=14, labelpad=-25)\n",
    "\n",
    "    # Tighter axis limits\n",
    "    ax.set_xlim(-50, 50)\n",
    "    ax.set_ylim(-50, 50)\n",
    "    ax.set_zlim(0, seq_len)\n",
    "\n",
    "    # Cleaner ticks\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12, colors='gray', pad=4)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Usage:\n",
    "fig = plot_latent_waterfall_clean(latents, outputs, show_text=False)\n",
    "\n",
    "# For saving:\n",
    "# fig.savefig(f'latent_waterfall_{m1[0]}_bright.png',\n",
    "#             dpi=300,\n",
    "#             bbox_inches='tight',  # This removes extra whitespace when saving\n",
    "#             pad_inches=0.1,       # Minimal padding around the plot\n",
    "#             facecolor='white',\n",
    "#             edgecolor='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_token_rotations_highlight(latents, token_texts, tokens_to_show, start_token=150, end_token=200, full_pca=True):\n",
    "    \"\"\"Publication-ready visualization of token evolution patterns\"\"\"\n",
    "    # Use exactly the same PCA computation as original\n",
    "    end_token = min(end_token, latents.shape[2])\n",
    "    chunk_latents = latents[:, 0, start_token:end_token]\n",
    "    num_steps = latents.shape[0]\n",
    "\n",
    "    # Get PCA components\n",
    "    if full_pca:\n",
    "        flat_latents = latents.reshape(-1, latents.shape[-1])\n",
    "        pca = PCA(n_components=6)\n",
    "        pca_proj = pca.fit_transform(flat_latents).reshape(num_steps, latents.shape[2], -1, 6)[:, start_token:end_token].reshape(num_steps, -1, 6)\n",
    "    else:\n",
    "        flat_latents = chunk_latents.reshape(-1, latents.shape[-1])\n",
    "        pca = PCA(n_components=6)\n",
    "        pca_proj = pca.fit_transform(flat_latents).reshape(num_steps, -1, 6)\n",
    "\n",
    "    # Convert absolute indices to relative positions within our window\n",
    "    relative_indices = [idx - start_token for idx in tokens_to_show]\n",
    "\n",
    "    # Create figure with square subplots\n",
    "    plt.style.use('default') # seaborn-v0_8-paper\n",
    "    fig, axes = plt.subplots(len(relative_indices), 3, \n",
    "                            figsize=(12, 4*len(relative_indices)))\n",
    "\n",
    "    if len(relative_indices) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    for row, rel_idx in enumerate(relative_indices):\n",
    "        abs_idx = tokens_to_show[row]  # Use actual token index for display\n",
    "        token_text = token_texts[abs_idx]\n",
    "\n",
    "        for comp_idx in range(3):\n",
    "            comp1, comp2 = 2*comp_idx, 2*comp_idx+1\n",
    "            ax = axes[row, comp_idx]\n",
    "\n",
    "            # Get trajectory\n",
    "            traj = pca_proj[:, rel_idx, [comp1, comp2]]\n",
    "            # Center trajectory at center of mass\n",
    "            center_of_mass = np.mean(traj, axis=0)\n",
    "            centered_traj = traj - center_of_mass\n",
    "            # Plot centered trajectory with improved styling\n",
    "            scatter = ax.scatter(centered_traj[:, 0], centered_traj[:, 1],\n",
    "                               c=np.arange(num_steps),\n",
    "                               cmap='viridis',\n",
    "                               s=25,\n",
    "                               alpha=0.7,\n",
    "                               rasterized=True)\n",
    "\n",
    "            # Connect points with lines\n",
    "            ax.plot(centered_traj[:, 0], centered_traj[:, 1], \n",
    "                   color='darkblue', alpha=0.3, linewidth=0.8)\n",
    "\n",
    "            # Mark the center of mass\n",
    "            ax.scatter([0], [0], c='r', s=80, marker='x', linewidth=2, zorder=5)\n",
    "\n",
    "            # Improved titles and labels\n",
    "            if comp_idx == 0:\n",
    "                ax.set_title(f'Token: \"{token_text}\"\\nPC{comp1+1}-PC{comp2+1}', \n",
    "                           fontsize=10, pad=8)\n",
    "            else:\n",
    "                ax.set_title(f'PC{comp1+1}-PC{comp2+1}', \n",
    "                           fontsize=10, pad=8)\n",
    "\n",
    "            # Refined grid and axes\n",
    "            ax.grid(True, linestyle='--', alpha=0.3)\n",
    "            ax.axhline(y=0, color='k', linestyle='-', alpha=0.15, linewidth=0.8)\n",
    "            ax.axvline(x=0, color='k', linestyle='-', alpha=0.15, linewidth=0.8)\n",
    "\n",
    "            # Set axis limits based on this subplot's data range\n",
    "            x_max = np.abs(centered_traj[:, 0]).max() * 1.2\n",
    "            y_max = np.abs(centered_traj[:, 1]).max() * 1.2\n",
    "            ax.set_xlim(-x_max, x_max)\n",
    "            ax.set_ylim(-y_max, y_max)\n",
    "\n",
    "            # Add minimal ticks\n",
    "            x_tick = int(np.ceil(x_max))\n",
    "            y_tick = int(np.ceil(y_max))\n",
    "            ax.set_xticks([-x_tick, 0, x_tick])\n",
    "            ax.set_yticks([-y_tick, 0, y_tick])\n",
    "            ax.tick_params(labelsize=8)\n",
    "\n",
    "            # Add colorbar with refined styling\n",
    "            # if comp_idx == 2:\n",
    "            #     cbar = plt.colorbar(scatter, ax=ax, label='Step', ticks=[0, num_steps-1])\n",
    "            #     cbar.ax.tick_params(labelsize=8)\n",
    "            #     cbar.set_label('Step', size=9)\n",
    "\n",
    "    plt.tight_layout(h_pad=2, w_pad=2)\n",
    "    return fig\n",
    "\n",
    "# Usage:\n",
    "if messages[1].content[0] == \"C\":\n",
    "    interesting_tokens = list(range(163, 180))\n",
    "elif messages[1].content[0] == \"W\":\n",
    "    interesting_tokens = list(range(182, 187))\n",
    "else: \n",
    "    interesting_tokens = list(range(88, 93))  # Use absolute indices\n",
    "fig_highlight = plot_token_rotations_highlight(latents, token_texts, \n",
    "                                             tokens_to_show=interesting_tokens,\n",
    "                                             start_token=0, end_token=250,\n",
    "                                             full_pca=True)\n",
    "# fig_highlight.savefig(f'swirlies_range_{m1[0]}_{interesting_tokens[0]}_{interesting_tokens[-1]}.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
