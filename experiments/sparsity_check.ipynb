{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d57d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GenerationConfig\n",
    "from recdep.utils.model import load_model_and_tokenizer\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e454ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Huginn tokenizer settings.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(\"tomg-group-umd\", \"huginn-0125\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "config = GenerationConfig(max_length=1024, stop_strings=[\"<|end_text|>\", \"<|end_turn|>\"], \n",
    "                          do_sample=False, temperature=None, top_k=None, top_p=None, min_p=None, \n",
    "                          return_dict_in_generate=True,\n",
    "                          eos_token_id=65505,bos_token_id=65504,pad_token_id=65509)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4fc5644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 7473/7473 [00:00<00:00, 481578.74 examples/s]\n",
      "Generating test split: 100%|██████████| 1319/1319 [00:00<00:00, 237243.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"openai/gsm8k\", \"socratic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3daded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(vals):\n",
    "    return tokenizer.apply_chat_template(vals['question'], tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "\n",
    "tokenized_dataset = ds.map(\n",
    "    tokenize_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db40f4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [65504, 65, 933, 1353, 2849, 402, 59416, 286, 4201, 6305, 295, 3434, 337, 2360, 5564, 6305, 46, 19779, 1523, 59416, 291, 1543, 1364, 431, 1972, 63], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, 'answer': {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [65504, 2395, 1523, 59416, 286, 5564, 6305, 1364, 431, 1972, 63, 935, 1147, 2849, 402, 47, 50, 61, 5539, 50, 47, 50, 61, 49, 4616, 49, 48351, 286, 5564, 6305, 10, 2395, 1523, 59416, 291, 1543, 1364, 431, 1972, 63, 935, 2127, 264, 1543, 3353, 286, 12026, 305, 402, 43, 49, 61, 5539, 50, 43, 49, 61, 51, 4616, 51, 59416, 286, 12026, 10, 1319, 532], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset['test'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be716c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_latents(model, outputs, num_steps=128):\n",
    "    # Get initial state and compute trajectory\n",
    "    embedded_inputs, _,_ = model.embed_inputs(outputs.sequences)\n",
    "    input_states = model.initialize_state(embedded_inputs, deterministic=False)\n",
    "\n",
    "    # Initialize storage for normalized latents\n",
    "    latents = []\n",
    "    current_latents = input_states\n",
    "    latents.append(model.transformer.ln_f(current_latents).cpu().float().numpy())\n",
    "\n",
    "    # Collect all latent states\n",
    "    for step in range(num_steps):\n",
    "        current_latents, _,_ = model.iterate_one_step(embedded_inputs, current_latents)\n",
    "        normalized_latents = model.transformer.ln_f(current_latents)\n",
    "        latents.append(normalized_latents.cpu().float().numpy())\n",
    "\n",
    "    # Stack all latents\n",
    "    latents = np.stack(latents)  # [num_steps+1, batch, seq_len, hidden_dim]\n",
    "    return latents\n",
    "\n",
    "\n",
    "latents = compute_latents(model, outputs, num_steps=128)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
